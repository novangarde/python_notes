
> Статья для изучения: [Знакомьтесь, линейные модели](https://habr.com/ru/articles/278513/)

[[#1. Понимание основ]]
	[[#Формула регрессии]]
	[[#Независимые переменные]]
	[[#Метод наименьших квадратов (МНК)]]
[[#2. Реализация модели]]
[[#3. Интерпретация результатов]]
	[[#P-value (P> t )]]
	[[#Коэффициент (slope)]]
	[[#Свободный член (intercept)]]
[[#4. Подходы к применению линейной регрессии]]
	[[#Эконометрический подход]]
	[[#Подход машинного обучения]]
## 1. Понимание основ

Линейная регрессия моделирует зависимость между независимыми переменными и зависимой переменной через линейное уравнение. Это включает знание формулы регрессии и механизма работы метода наименьших квадратов для оценки коэффициентов

Используется, когда надо создать рейтинг. К примеру, нам нужно как-то ранжировать между собой сотрудников по качеству их работы. 
1. Перечисляем список сотрудников.
2. Добавляем какие-то характеристики.
3. Интуитивно ранжируем их.
4. Отдаем результаты в линейную регрессию, а она уже расставляет "веса" каждого фактора.

### Формула регрессии

$$Y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n + \epsilon$$
где $x_j=f(x_j)$, $Y = g(Y)$
$Y$ - целевая переменная, таргет, зависимая переменная
$b$ - коэффициенты
$x$ - независимые переменные, предикторы, факторы, регрессоры
$\epsilon$ - случайная величина, свободный член

Мы предполагаем, что связь между $Y$ и $x$ линейная. Единственное, что мы не знаем, какие коэффициенты и какой-то свободный член, который нам тоже нужен.

Модель считается линейной, потому что она линейно зависит от параметров $b$.

Обычно n > 1, а $Y^*$ - скаляр

### Независимые переменные

> Мультиколлинеарность - переменные $x$ должны быть линейно независимы между собой
> Коллинеарность - проблема для эконометристов, не проблема для дата-сайентистов

* **Бинарные** - Да/Нет.
* **Категориальные** - Регионы страны.
* **Упорядоченно-категориальные** - Оценки на экзамене, рейтинг.
* **"Контрольные"** - регрессия показывает влияние какого-то признака при учете всех остальных.
* **Описывающие убывающие функции** - можем взять $x^2$ или $\sqrt x$, и получится либо возрастающая, либо убывающая функция.

![[Pasted image 20250222212241.png]]


### Метод наименьших квадратов (МНК)

Метод наименьших квадратов (МНК) — это статистический метод, используемый для оценки параметров моделей, основанный на минимизации суммы квадратов отклонений между наблюдаемыми значениями и предсказанными значениями модели. Этот метод широко применяется в регрессионном анализе и позволяет находить наилучшие параметры для линейных и нелинейных моделей.

Суть метода заключается в том, что для заданного набора данных $(x_i, y_i)$, где: $y_i$ - это наблюдаемые значения, а $x_i$ - независимые переменные, мы хотим найти $f(x, \beta)$, которая будет описывать зависимость $y$ от $x$. Параметры $\beta$ выбираются так, чтобы минимизировать сумму квадратов ошибок:

$$S(\beta)=\sum_{i=1}^n(y_i - f(x_i, \beta))^2$$
где:
$S(\beta)$ - функция потерь
$n$ - количество наблюдений
$y_i$ - фактическое значение
$f(x_i,\beta)$ - предсказанные значения модели

В простейшем случае, когда функция имеет линейный вид
$$f(x)=\beta_0 + \beta_1x$$
где:
$\beta_0$ - свободный член (константа)
$\beta_1$ - коэффициент наклона

Тогда задача сводится к минимизации следующей функции:
$$S(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_1))^2$$
Для нахождения оптимальных значений параметров $\beta_0$ и $\beta_1$, необходимо взять частные производные функции потерь по каждому из параметров, приравнять их к нулю и решить полученную систему уравнений.

### Выбор линейной регрессии

Чтобы понять, что мы должны строить именно модель линейной регрессии мы должны предположить, что:
1. Существует линейная связь между $Y$ и $x$-ми.
2. Выборка у нас репрезентативна.
3. **Строгая экзогенность** - мы уверены, что выбранные факторы $x$ влияют на $Y$.

$x$ называют экзогенными, если ошибка модели никак не влияет на $x$ (и $Y$).
Экзогенность подразумевает, что мы подобрали корректные факторы для вычисления зависимой переменной.

### Добавление константы

Добавление константы в контексте линейной регрессии и других моделей статистического анализа — это важный шаг, который позволяет учесть свободный член (или интерсепт) в модели.

Свободный член (интерсепт) — это значение, которое модель предсказывает, когда все независимые переменные равны нулю. Это позволяет модели корректно учитывать базовый уровень зависимости между переменными.

Если вы не добавите свободный член, модель будет принуждена проходить через начало координат (точка (0, 0)), что может привести к неправильным предсказаниям и снижению качества модели.

Функция `sm.add_constant()` добавляет новый столбец к DataFrame или матрице с данными, который содержит только единицы. Это делается для того, чтобы обеспечить наличие свободного члена в модели. После добавления константы с помощью `sm.add_constant(X1_numeric)` в датафрейме появится новый столбец `const`, который содержит только единицы и представляет собой свободный член для модели.

```Python
X1_with_const = sm.add_constant(X1_numeric) # добавляет константу к датафрейму X1
```

## 2. Реализация модели

Умение использовать библиотеки, такие как scikit-learn, для построения и обучения модели линейной регрессии — это важный навык. Вы должны уметь писать код для разделения данных на обучающую и тестовую выборки, а также для оценки производительности модели с помощью метрик, таких как среднеквадратичная ошибка (MSE) или R-квадрат.

## 3. Интерпретация результатов

Важно уметь интерпретировать результаты вашей модели, включая понимание значимости коэффициентов и их влияния на предсказания.

### P-value (P>|t|)

Если `p-value` 0.345 для коэффициента при `feature`, значит связи между независимой и зависимой переменными нет. Это верно, если p-value больше 0.05 (обычно используется уровень значимости в 5%).

### Коэффициент (slope)

Если коэффициент (или наклон) равен 6.0658, это означает, что при увеличении значения независимой переменной `feature` на 1 единицу, стоимость товара (`cost`) увеличивается в среднем на 6.0658 рублей.

**Интерпретация**: Например, если характеристика товара увеличивается с 10 до 11, то стоимость товара, согласно вашей модели, увеличится примерно на 6.07 рублей.

### Свободный член (intercept)

Свободный член (или константа) равен 9765.4801. Это значение представляет собой предсказанную стоимость товара, когда значение независимой переменной `feature` равно 0.

**Интерпретация**: В вашем случае, если характеристика товара равна 0, то модель предсказывает, что стоимость товара будет равна примерно 9765.48 рублей. Однако важно учитывать, что в реальной жизни значение `feature` может не достигать нуля, и интерпретация свободного члена может быть менее значимой.

### R² score

**Коэффициент детерминации**. Это мера того, насколько хорошо модель объясняет вариацию зависимой переменной. Он принимает значения от 0 до 1, где:
- **0** означает, что модель не объясняет вариацию данных.
- **1** означает, что модель полностью объясняет вариацию данных.

Формула для вычисления R²:
$$R^2=1-\frac{\Sigma(y_i-\hat{y})^2}{\Sigma(y_i-\overline{y})^2}$$
где:
$y_i$ - истинные значения
$\hat{y}$ - предсказанные значения
$\overline{y}$ - среднее значение истинных значений

**Пороги для оценки R²**:
- **Низкое качество**: R²<0.5R²<0.5 — модель считается плохой, так как она не объясняет значительную часть вариации.
- **Удовлетворительное качество**: R²>0.5R²>0.5 — модель удовлетворительная, так как она объясняет более половины вариации.
- **Высокое качество**: R²>0.8R²>0.8 — модель считается очень хорошей, так как она объясняет большую часть вариации в данных.

К примеру R² = 0.278 будет означать, что модель угадывает только 27.8% случаев - это довольно низкий показатель, что говорит о том, что модель плохо справляется с предсказанием цен. А R² = 0.681 объясняет 68.2% вариации цен, что является значительно лучшим результатом и указывает на более высокую предсказательную способность.

### RMSE

RMSE расшифровывается как **корень среднеквадратичной ошибки** (Root Mean Squared Error). Это метрика, используемая для измерения точности прогнозов модели в статистике и анализе данных. RMSE количественно определяет разницу между предсказанными значениями и фактическими наблюдаемыми значениями. Она выражается в тех же единицах, что и целевая переменная, что делает ее интерпретацию более интуитивной.

Формула:
$$RMSE = \sqrt {\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
где:
$n$ - количество наблюдений
$y_i$ - истинные значения
$\hat{y}_i$ - предсказанные значения
$i=n$ - нижний предел суммы, который указывает, что мы начинаем суммировать с первого наблюдения.
$n$ - верхний предел суммы, который указывает, что мы заканчиваем суммирование на последнем наблюдении

- **Расчет**: RMSE вычисляется как квадратный корень из среднего значения квадратов разностей между предсказанными и истинными значениями. Это позволяет избежать отрицательных значений и акцентировать внимание на больших ошибках.
- **Интерпретация**: Более низкое значение RMSE указывает на лучшее соответствие модели данным, что означает, что предсказания модели ближе к фактическим значениям.
- **Единицы измерения**: RMSE измеряется в тех же единицах, что и целевая переменная, что делает его интерпретацию более интуитивной.

### MAPE

MAPE расшифровывается как **средняя абсолютная процентная ошибка** (Mean Absolute Percentage Error). Измеряет точность предсказаний в процентах и позволяет понять, насколько близки предсказания к истинным значениям относительно величины самих значений.

Формула для MAPE:

$$MAPE = \frac{100}{n} \sum_{i=1}^n \left| {\frac{y_i - \hat{y_i}}{y_i}} \right|$$
где:
$y_i$ - истинные значения
$\hat{y}_i$ - прогнозируемые значения
$n$ - количество наблюдений

**Интерпретация**: Значение MAPE выражается в процентах и позволяет легко интерпретировать точность модели. Например, если MAPE составляет 10%, это означает, что в среднем ошибки прогноза составляют 10% от фактических значений.

## 4. Подходы к применению линейной регрессии

**Есть два подхода:**
1. Экономический подход (научный) - проверка гипотез
2. Подход машинного обучения - прогноз

### Эконометрический подход

Есть теория реальности, которую мы описываем, исходя из каких-то наблюдений. Например:

> При увеличении цены падает количество покупок

Нам нужно оценить теоретические связи: собирая определенным образом данные, мы можем оценить эффект, который оказывает увеличение цены на уменьшение количества покупок.

**Решение:** Регрессионный анализ МНК.

### Подход машинного обучения

Есть данные, мы можем собрать другие данные. Например:

> У нас есть данные о турникетах, через которые проходят наши посетители.

Мы хотим выяснить, не отдал ли Петя свой ключ к турникету кому-то из коллег. Зная систему посещения офиса всех коллег, мы для каждого "подозрительного" посещения можем предсказать, кто это был, на основании истории посещений.

**Решение:** Линейная регрессия с регуляризацией.










## Решение

Из-за случайностей $\epsilon$ мы не можем легко рассчитать искомые коэффициенты $b$ нашей модели.

Решаем оптимизационную задачу вида:

$$b^* = argmin F(b | X, Y)$$
где
$F$ - некий функционал ("ошибка алгоритма", функция потерь, loss function).

Наша задача уменьшать $F$, например $\Sigma \epsilon^2$, сумму квадратов ошибки, но можно задать и другие.

## Решение МНК графически: линейный тренд

> МНК - метод наименьших квадратов

![[Pasted image 20250222213349.png]]







**Какие коэффициенты с помощью машинного обучения подбираются?**
Подбираются веса, смещения и параметры регуляризации, которые минимизируют функцию потерь.


**Как работает dropout?**


**Какие методы регуляризации есть?**
К популярным методам регуляризации относятся:
-  L1 (Lasso): добавляет сумму абсолютных значений коэффициентов к функции потерь, что помогает обнулить ненужные параметры.
-  L2 (Ridge): добавляет сумму квадратов коэффициентов, уменьшая их значения.
-  Dropout: отключает случайные нейроны в процессе обучения, снижая переобучение.
-  ElasticNet: комбинация L1 и L2.