
Программная библиотека на языке Python, предназначенная для обработки и анализа структурированных данных. Она особенно полезна для работы с табличными данными, такими как CSV-файлы или Excel-таблицы.

**DataFrame** — это одна из основных структур данных в библиотеке pandas, предназначенная для работы с табличными данными. Его можно представить как двумерную таблицу, аналогичную таблицам в базах данных или листам в Excel.

```Python
import pandas as pd
data = {
    'Имя': ['Анна', 'Борис', 'Виктор'],
    'Возраст': [28, 34, 29],
    'Город': ['Москва', 'Санкт-Петербург', 'Новосибирск']
}
df = pd.DataFrame(data)
```

**DataFrame можно создать из словаря**, у которого в ключах строковые значения колонок, а в значениях - серии или списки. То есть, если мы предоставим такой словарь методу `pd.DataFrame(<наш словарь>)`, то конструктор преобразует `Series` и списки в единообразные `Series` внутри `DataFrame`.

**Series** — это одномерная структура данных Pandas.

### pd.read_csv

Метод для чтения файлов формата .csv, файлов, в которых столбцы отделены между собой табуляцией. Пример:

```Python
import pandas as pd

filepath = "../data/feed-views.log"

df = pd.read_csv(filepath, skiprows=[2, 3], skipfooter=2, engine='python', header=None, names=['datetime', 'user'])

# skiprows - пропускает строки под индексами 2 и 3 (в нашем примере)

# skipfooter - пропускает последние две строки (в нашем случае)

# engine='python' - указываем движок `Python`, потому что когда вы используете аргументы `skiprows` и `skipfooter`, Python-движок часто является обязательным для корректной работы этих функций. Это связано с тем, что движок Python позволяет более гибко обрабатывать строки и футер.

# header=None, names=['date_time', 'user'] - указывает на отсутствие заголовков в файле и сразу добавляет их в виде `date_time` и `user`.
```

После прочтения файл сохраняется в переменную `df`, с которой мы дальше можем работать, изменяя заголовки, назначая индексы и т.д.

> Также при чтении файла может понадобиться указать параметр `sep='\t'`, чтобы метод разделил колонки по разделителю, иначе вся строка может попасть в одну колонку. 

### df.set_index()

Вернемся к примеру выше и установим одну из колонок в качестве индекса

```Python
df.set_index('datetime', inplace=True)

# Устанавливает колонку datetime индексной колонкой.

# Параметр `inplace=True` означает, что метод изменяет исходный DataFrame напрямую без создания нового объекта. Если вы не используете параметр `inplace=True`, метод возвращает новый измененный DataFrame, а исходный остается неизменным.
```

После установления столбца `dataframe` индексным, этот столбец становится индексом DataFrame. После этого вы можете обращаться к значениям по ключу из индексной колонки.

### df.reset_index()

Если какая-либо колонка установлена в качестве индекса, ее нельзя переименовать. Поэтому сначала придется снять индекс при помощи `reset_index()`

```Python
df.reset_index(inplace=True)

# inplace=True означает, что метод изменяет исходный DataFrame напрямую без создания нового объекта.
```

### df.rename()

При помощи этого метода мы можем изменять название колонок

```Python
df.rename(columns={'datetime': 'date_time'}, inplace=True)

# inplace=True означает, что метод изменяет исходный DataFrame напрямую без создания нового объекта.
```

Если колонка установлена индексом, изменить ее не получится. Придется сначала использовать метод [[#df.reset_index()]], затем переименовать и потом снова установить индекс при помощи [[#df.set_index()]].

### df.drop_duplicates()

Используется для удаления дубликатов строк из DataFrame. Он позволяет оставить только уникальные комбинации значений в указанных столбцах.

```Python
df.drop_duplicates(subset=['CarNumber', 'Make_n_model', 'Fines'], keep='last')

# subset содержит список колонок, в которых нужно удалить дубликаты
# keep='last' устанавливает правило, что при удалении сохраняем последний дубль
```

### df.to_csv()

Объект pandas можно сохранить в .csv формате и сразу установить разделитель

```Python
df.to_csv('output.csv', sep=';')

# Параметр sep (separator) - устанавливает разделитель между колонками
```

### df.to_excel()

```Python
df.to_excel('ваш_файл.xls', index=False)
```

### df.to_json()

```Python
df.to_json('../data/auto.json', orient='records')
```

**`orient='records'`**: Этот параметр указывает Pandas сохранять данные так, чтобы каждая строка была представлена как отдельный словарь (запись). Это соответствует вашему желаемому формату.

### pd.to_datetime()

Этот метод преобразует выбранный столбец в формат `datetime64[ns]`, формат даты и времени, принятый в Pandas и NumPy:

```Python
df['datetime'] = pd.to_datetime(df['datetime'])

# df['datetime'] указывает на нужный столбец в объекте
```

После преобразования мы можем создать в объекте новые столбцы, присваивая им значение из вложенного объекта dt и соответствующего атрибута:

```Python
df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['hour'] = df['datetime'].dt.hour
df['minute'] = df['datetime'].dt.minute
df['second'] = df['datetime'].dt.second
```

### pd.cut()

Функция в библиотеке Pandas, которая используется для разделения непрерывных числовых данных на дискретные интервалы или категории. Она позволяет преобразовать непрерывные переменные в категориальные, что может быть полезно для анализа данных, создания гистограмм или сегментации пользователей.

#### Основные параметры и возможности:

- **bins**: Определяет границы интервалов. Может быть массивом чисел или количеством равных интервалов.
- **labels**: Присваивает метки каждому интервалу.
- **right**: Указывает, включать ли правую границу каждого интервала (по умолчанию `True`).
- **include_lowest**: Позволяет включить самую низкую точку в первый интервал (по умолчанию `False`).

#### Пример

```Python
bins = [0, 4, 7, 11, 17, 20, 24]

labels = ['night', 'early morning', 'morning', 'afternoon', 'early evening', 'evening']

df['daytime'] = pd.cut(df['hour'], bins=bins, labels=labels)
```

В этом примере, данные колонки hour будут разбиты на интервалы от 0 до 3, от 4 до 6, от 7 до 10, от 11 до 16, от 17 до 19, от 20 до 23 при помощи `bins`

Затем, при помощи `labels` каждому из интервалов присвоится нужное значение и сохранится в новую колонку `daytime`.

### df.count()

Используется для подсчета количества не пропущенных значений в каждом столбце DataFrame. Он игнорирует пропущенные значения

```Python
df.count()
# выводит количество значений в каждом столбце
```

### df.value_counts()

Используется для подсчета частоты каждого уникального значения в Series или столбце DataFrame. Он возвращает Series, где индексами являются уникальные значения из исходных данных, а значениями — количество раз, когда каждое из этих значений встречается.По умолчанию результаты сортируются по убыванию частоты (самые часто встречающиеся значения первыми). Пропущенные значения исключаются из результата по умолчанию.

```Python
df["column"].value_counts()
# Возвращает уникальные значения из колонки "column" с количеством их вхождений
```

### df.sort_values()

Используется для сортировки DataFrame или Series по значениям в указанном столбце или столбцах. Он позволяет сортировать данные как по возрастанию, так и по убыванию.

```Python
df.sort_values(by=['hour', 'minute', 'second'], ascending=[True]*3, inplace=True)

# сортирует по часам, минутам и секундам, по возрастанию
# вместо ascending=[True]*3 может быть ascending=[True, True, True] - в таком формате понятнее, как сортировать по разным столбцам
```

### df.loc()

Используется для доступа к строкам и столбцам DataFrame по их меткам. Он позволяет выбирать данные на основе логических условий, что делает его мощным инструментом для фильтрации данных.

#### Принцип работы

**Выбор по меткам**: Вы можете использовать `loc` для доступа к конкретным строкам или столбцам, указывая их метки.

```Python
df.loc['row_label', 'column_name']
```

**Выбор нескольких строк или столбцов**: Можно передавать списки меток для выбора нескольких строк или столбцов одновременно.

```Python
df.loc[['row1', 'row2'], ['col1', 'col2']]
```

**Логические условия**: Также можно использовать логические условия (булевы массивы) для фильтрации данных.

```Python
df.loc[df['column_name'] > value]
```

#### Примеры

**С одним условием:**

```Python
df.loc[df['Fines'] > 2100]
# находит все записи, в которых штрафы больше 2100
```

**С несколькими условиями:**

```Python
df.loc[(df['Fines'] > 2100) & (df['Refund'] == 2)]
# находит все записи, в которых штрафы больше 2100 и где refund равен 2
# условия объединяются амперсандом
# условия должны быть заключены в скобки
```

**Вывод строк, значения которых совпадают со значениями из списка:**

```Python
df.loc[df['Model'].isin(['Focus', 'Corolla'])]
```

**С применением условий к индексному столбцу:**

```Python
df.loc[df.index.isin(['Y7689C197RUS', '92928M178RUS', '7788KT197RUS', 'H115YO163RUS', 'X758HY197RUS'])]

# df.index - обращается к индексной колонке
```

### df.nsmallest()

Возвращает n самых маленьких значений из указанной колонки.

```Python
df.nsmallest(3, columns='hour')
# вернет три самых маленьких значения из колонки hour
```

Возможен и комбинированный вариант:

```Python
earliest_df = df[(df['daytime'] == 'morning')].nsmallest(3, columns='hour')[["hour"]]

# в earliest_df сохранится ттри самых маленьких значения из колонки hour, но будет выведена только индексная колонка и колонка "hour" с заголовком.
```

### df.nlargest()

Возвращает n самых больших значений из указанной колонки.

```Python
df.nlargest(3, columns='hour')
# вернет три самых больших значения из колонки hour
```

Комбинированный вариант:

```Python
latest_df = df[(df['daytime'] == 'morning')].nlargest(3, columns='hour')[["hour"]]

# в earliest_df сохранится три самых больших значения из колонки hour, но будет выведена только индексная колонка и колонка "hour" с заголовком.
```

### .df.describe()

Используется для генерации сводной таблицы с основными статистическими показателями для числовых столбцов DataFrame.

Пример:

```Python
stats = df['hour'].describe()
print(f"Basic statistics:\n{stats}")

# вернет статистические показатели по колонке hour
```

Статистика от `describe()`:

| Значение | Описание                                          |
| -------- | ------------------------------------------------- |
| count    | количество значений в колонке                     |
| mean     | медианное значение                                |
| min      | минимальное                                       |
| 25%      | значение на рубеже первого и второго квартиля     |
| 50%      | значение на рубеже второго и третьего квартиля    |
| 75%      | значение на рубеже третьего и четвертого квартиля |
| max      | максимальное                                      |

### df.isnull() || df.isna()

Проверяет, является ли ячейка пустой. Если да, возвращает True, если нет - False

```Python
df.isnull()
# Вернет таблицу, заполненную True/False значениями
```

Может быть использована для подсчета количества пустых ячеек, если применить вместе с `sum()` (не сработает с `count()`, который просто посчитает количество строк с True/False, поэтому вернет всю длину таблицы):

```Python
df.isnull().sum()
# вернет количество пустых ячеек в каждом столбце (суммирует True)
```

### df.dropna()

Используется для удаления строк или столбцов из DataFrame, содержащих пропущенные значения (NaN/NA)

```Python
df = df.dropna(axis=1, thresh=len(df)-500)
# axis - по умолчанию 0, удаляет строки, если поставить 1 - столбцы
# thresh - указывает минимальное число заполненных ячеек, которому должна соответствовать аггрегация, чтобы колонку не удалили. В данном примере, мы из длины таблицы вычитаем 500, то есть, удаляем все колонки, в которых больше 500 пустых ячеек.
```

### df.fillna()

Используется для заполнения пропущенных значений (NaN/NA) в DataFrame или Series

```Python
df['Refund'] = df['Refund'].fillna(method='ffill')
# method='ffill' (forward fill) означает, что ячейка будет заполнена значением из предыдущей ячейки. Можно просто указать df.fillna(0), чтобы заменить все на 0. Можно заменить на bfill (backward fill)
```

### df.apply()

Используется для применения пользовательской функции к каждому элементу, столбцу или строке DataFrame. Он позволяет выполнять операции над данными без использования циклов, что делает код более компактным и читаемым.

#### Принцип работы

- **Применение к столбцам или строкам**: Метод `apply()` может применяться как к столбцам (`axis=0`), так и к строкам (`axis=1`) DataFrame. Это зависит от значения параметра `axis`.
- **Обработка данных**: Передаваемая функция принимает Series (в случае обработки по столбцам) или ряды (при обработке по строкам) и выполняет над ними необходимые действия.
- **В передаваемую функцию передается value**: когда мы вызываем функцию для обработки значений колонки или строки, в параметры этой функции передается переменная `value`, с которой и нужно работать. Она, как в цикле, обрабатывает строки/колонки.
- **Вернуть нужно pd.Series:** передаваемая функция должна возвращать `pd.Series()`, потому что все ряды/колонки обрабатываются циклами и сохраняют серии.

```Python
def split_make_model(value):
    parts = value.split()
    if len(parts) >= 2:
        make = parts[0]
        model = ' '.join(parts[1:])
    else:
        make = parts[0]
        model = ''
    return pd.Series({'Make': make, 'Model': model})

df[['Make', 'Model']] = df['Make_n_model'].apply(split_make_model)
```

### df.groupby()

Позволяет группировать данные по одному или нескольким столбцам и выполнять агрегационные операции над этими группами. Это особенно полезно для анализа данных, когда нужно изучить различные аспекты данных или сделать сводные выводы на основе группировки.

После группировки можно применять различные агрегационные функции к каждой группе, такие как:

- `mean()`: Среднее значение.
- `median()`: Медиана.
- `sum()`: Сумма.
- `count()`: Количество элементов.
- `min()` и `max()`: Минимальное и максимальное значения.

#### Группировка по одному столбцу

```Python
df.groupby('Make')['Fines'].median()

# Группируем по столбцу 'Make'
# Находим медиану из значений колонки 'Fines'
```

#### Группировка по двум столбцам

```Python
df.groupby(['Make', 'Model'])['Fines'].median()

# Группируем по столбцам 'Make' и 'Model'
# Находим медиану из значений колонки 'Fines'
```

#### Подсчет количества значений в группировке

```Python
df.groupby(['Make', 'Model'])['Fines'].count()

# Группируем по столбцам 'Make' и 'Model'
# Находим количество значений колонки 'Fines'
```

#### Рассчитываем дисперсию

```Python
df.groupby(['Make', 'Model'])['Fines'].std()

# Группируем по столбцам 'Make' и 'Model'
# Находим дисперсию значений колонки 'Fines'
```

#### Рассчитываем дисперсию

```Python
df.groupby(['Make', 'Model'])['Fines'].std()

# Группируем по столбцам 'Make' и 'Model'
# Находим дисперсию значений колонки 'Fines'
```

### df.agg()

Используется для применения агрегирующих функций к данным. Он позволяет вычислять сводные значения, такие как сумма (`sum`), среднее (`mean`), медиана (`median`), количество элементов (`count`) и другие, для групп данных или отдельных столбцов DataFrame.

```Python
df.groupby(['Make', 'Model'])['Fines'].agg(['min', 'max'])

# Группируем по столбцам 'Make' и 'Model'
# Находим минимальные и максимальные значения колонки 'Fines' благодаря аггрегированной функции.
```

### pd.set_option()

Используется для установки различных параметров, которые влияют на поведение и отображение данных. Эти параметры могут быть связаны с вычислениями, отображением данных или другими аспектами работы с DataFrame.

**Параметры**: Существует множество доступных параметров, которые можно изменить с помощью `.set_option()`. Например:

* `display.max_rows`: Количество строк для отображения.
* `display.max_columns`: Количество столбцов для отображения.
* `display.float_format`: Формат вывода чисел с плавающей запятой.

**Синтаксис**:
```Python
pd.set_option('parameter_name', value)
```

**Пример:**
```Python
pd.set_option('display.float_format', lambda x: '%.2f' % x)
# устанавливает для всех float-значений отображение дробей до сотых
```

Сбрасывается методом [[#pd.reset_option()]]

### pd.reset_option()

Сбрасывает опции, установленные в [[#pd.set_option()]]

```Python
pd.reset_option('display.float_format')
# сбросит настройки, установленные для float
```

### df.sample()

Позволяет получить случайную подвыборку из DataFrame или Series. Он используется для выбора определенного количества строк или доли данных из исходного набора.

В NumPy есть [[NumPy#^np.random.seed(<число>)|метод]], который работает похожим образом.

```Python
df.sample(n=None, frac=None, replace=False, weights=None)

# n - Количество строк для выборки.
# frac - Доля данных для выборки (например, 0.5 для половины данных).
# replace - Позволяет повторно выбирать строки (`True`) или нет (`False`).
# weights - Вектор весов для каждого элемента (должен быть нормализован).
```

**Пример:**

```Python
df = df.sample(n=200, replace=True, random_state=21)

# n - создаем новую выборку из 200 случайных записей
# replace - означает, что будем использовать записи повторно
# random_state - используется для обеспечения воспроизводимости результатов
```

**Несколько слов о random_state**:

* **Псевдослучайные числа**: Когда вы вызываете метод `.sample()`, Pandas использует генератор псевдослучайных чисел для выбора строк из DataFrame. Эти числа не являются истинно случайными, а скорее предопределенной последовательностью чисел.
* **Начальное состояние**: Параметр `random_state` задает начальное состояние этого генератора. Если вы используете одно и то же значение для `random_state`, то каждый раз при запуске кода будет получаться одна и та же последовательность псевдослучайных чисел.
* Использование `random_state=21` означает, что каждый раз при вызове: `unique_combinations.sample(n=200, replace=True, random_state=21)` вы будете получать одну и ту же выборку из 200 строк из вашего набора данных (`unique_combinations`). Это полезно для отладки или сравнения результатов разных экспериментов.
* Если бы вы не указали значение для `random_state`, то каждая новая выборка была бы другой (если только вы явно не установите seed NumPy с помощью функции `np.random.seed()`).

### pd.concat()

Используется для объединения объектов Pandas (например, DataFrame или Series) вдоль определенной оси. Эта ось может быть либо по строкам (`axis=0`), либо по столбцам (`axis=1`).

**Пример:**

```Python
con_cat_rows_df = pd.concat([df, new_df], ignore_index=True)

# ignore_index=True используется, если исходные индексы не имеют значения или вы хотите сбросить их на последовательность
```

### pd.series()

Используется для создания одномерного массива данных с метками или индексами. Это одна из основных структур данных в Pandas, наряду с DataFrame.

```Python
import pandas as pd

# Создание Series из списка
data = pd.Series([1, 2, 3])
print(data)

# Создание Series со своим индексом
indexed_data = pd.Series([10, 20], index=['a', 'b'])
print(indexed_data)

# Создание Series из словаря
dict_data = {'x': 1000}
series_from_dict = pd.Series(dict_data)
print(series_from_dict)
```

### df\['col'].unique()

Возвращает уникальные значения из Series или DataFrame. Однако если вы вызываете `df.unique()` напрямую на DataFrame (а не на отдельной Series), то он вернет ошибку, так как этот метод предназначен для работы с одномерными данными.

```Python
df['CarNumber'].unique()
```

### pd.merge()

Используется для объединения двух DataFrame по общему столбцу или набору столбцов. Это мощный инструмент для работы с данными, позволяющий выполнять различные типы соединений между таблицами.

**Основные параметры**

- **`left` и `right`**: Левый и правый DataFrame для объединения.
- **`on`, `left_on`, `right_on`**: Колонки, по которым будет происходить соединение.
	* Если колонки имеют одинаковые названия в обоих DataFrame, то можно использовать просто `on`.
    - Если названия колонок разные, то используйте `left_on` и `right_on`.

- **`how='inner'`, `'left'`, `'right'`, `'outer'`: Тип соединения**:    
    - **Inner Join**: Возвращает только строки с совпадающими значениями в обоих DataFrame.
    - **Left (Outer) Join**: Возвращает все строки из левого DataFrame плюс соответствующие им строки из правого. Если нет совпадений — значения будут NaN.
    - **Right (Outer) Join**: Аналогично левому, но берет все строки из правого DataFrame.
    - **Full Outer Join (`how='outer'`)`: Возвращает все возможные комбинации строк из обоих наборов данных.

```Python
inner_join_result = pd.merge(fines[['CarNumber']], owners[['CarNumber', 'SURNAME']], on='CarNumber')

left_join_result = pd.merge(fines[['CarNumber']], owners[['CarNumber', 'SURNAME']], how='left', on='CarNumber')

right_join_result = pd.merge(fines[['CarNumber']], owners[['CarNumber', 'SURNAME']], how='right', on='CarNumber')

outer_join_result = pd.merge(fines[['CarNumber']], owners[['CarNumber', 'SURNAME']], how='outer', on='CarNumber')
```

### pd.pivot_table()

Используется для преобразования данных из длинной формы в широкую, создавая сводные таблицы. Она позволяет агрегировать данные по различным категориям и применять различные функции агрегации (например, сумма, среднее значение) к значениям.

**Основные параметры**

- **`index`**: Указывает столбцы или индекса DataFrame, которые будут использоваться как строки результирующей таблицы.
- **`columns`**: Определяет столбцы или категории данных, которые станут новыми столбцами в сводной таблице.
- **`values`**: Указывает на те данные, которые будут агрегированы и заполнены в ячейках новой таблицы.
- **`aggfunc='sum'`, `'mean'`, `'max'`, etc.**`: Функция агрегации для обработки дублирующихся записей.

**Пример:**

Был DataFrame `fines`:

```Python
CarNumber Refund Fines Make Model Year
0 Y163O8161RUS 2 3200.00 Ford Focus 1989.00
1 E432XX77RUS 1 6500.00 Toyota Camry 1995.00
2 7184TT36RUS 1 2100.00 Ford Focus 1984.00
3 X582HE161RUS 2 2000.00 Ford Focus 2015.00
4 92918M178RUS 1 5700.00 Ford Focus 2014.00
```

Я перегруппировал данные таким образом:

```Python
grouped_fines = fines.groupby(['Make', 'Model', 'Year'])['Fines'].sum().reset_index()
```

Теперь вывод `grouped_fines` выглядит так:

```Python
Make Model Year Fines
0 Audi 2003.004200.00
1 BMW 1987.00 2200.00
2 BMW 2012.00 3000.00
3 BMW 2015.00 8594.59
4 BMW 2018.00 6500.00
```

Из этого состояния я могу вызвать `pivot_table()`, чтобы пересобрать таблицу нужным мне образом:

```Python
pivoted_fines = pd.pivot_table(grouped_fines,
                               index=['Make', 'Model'],
                               columns='Year',
                               values='Fines',
                               aggfunc='sum')
```

Теперь у нас есть DataFrame `pivoted_fines` следующего содержания:

```Python
Year           1980.00 1981.00 1982.00 1983.00 1984.00 1985.00
Make Model
Audi           NaN     NaN     NaN     NaN     NaN     NaN
BMW            NaN     NaN     NaN     NaN     NaN     NaN
Ford Focus     82094.59 425489.17 160583.76 55900.00 97289.17 121183.76
	 Mondeo    NaN     NaN     NaN     NaN     NaN     NaN
Skoda Octavia  1900.00 NaN 6900.00 11594.59 NaN 10294.59
Toyota Camry   13500.00 8594.59 NaN 7200.00 NaN NaN
	   Corolla NaN     NaN     2000.00 1100.00 NaN     NaN
Volkswagen     NaN     NaN     NaN     NaN     NaN     NaN
	   Golf    30900.00 NaN    NaN     8594.59 300.00  24000.00
	   Jetta   NaN     NaN     NaN     NaN     NaN     NaN
	   Passat  600.00  1600.00 NaN     3200.00 10000.00 5000.00
	   Touareg NaN     NaN     NaN     NaN     NaN     5800.00
Volvo          NaN     NaN     6800.00 NaN     NaN     NaN
```

### df.copy()

Создает копию DataFrame. По умолчанию (`deep=True`) он производит глубокую копию, что означает создание нового объекта с отдельной копией данных и индексов. Это позволяет изменять копию независимо от оригинала и наоборот.

**Основные особенности**

1. **Глубокая копия**:
    
    - Когда `deep=True`, изменения в оригинале не влияют на копию.
    - Этот режим является стандартным для `copy()`.
    
2. **Поверхностная (мелкая) копия**:
    
    - Если указать `deep=False`, то будет скопирована только ссылка на данные.
    - Изменения в одном объекте повлияют на другой.
    
3. **Применение**:
    
    - Используйте для создания независимых версий DataFrame, чтобы избежать непредвиденных изменений исходных данных

```Python
new_df = df.copy()
```

